{
  "version": 1,
  "models": [
    {
      "id": "test-smollm3-3b-gguf",
      "name": "SmolLM3 3B (GGUF) - TEST MODEL",
      "repo_id": "ggml-org/SmolLM3-3B-GGUF",
      "file_path": "smollm3-3b-Q4_K_M.gguf",
      "model_type": "gguf",
      "size_gb": 1.8,
      "tags": ["test", "small", "fast"],
      "suggested": false,
      "downloaded": false,
      "description": "Small 3B GGUF model for testing - fast download and inference",
      "default_quant": "q4_k_m",
      "source": "test"
    },
    {
      "id": "test-bitnet-2b",
      "name": "BitNet b1.58 2B (4T) - TEST MODEL",
      "repo_id": "microsoft/bitnet-b1.58-2B-4T",
      "file_path": null,
      "model_type": "bitnet",
      "size_gb": 0.3,
      "tags": ["test", "bitnet", "ultra-fast", "efficient"],
      "suggested": false,
      "downloaded": false,
      "description": "Microsoft BitNet 2B for testing - extremely efficient 1.58-bit quantization",
      "default_quant": "i2_s",
      "source": "test"
    },
    {
      "id": "test-smollm3-3b-onnx",
      "name": "SmolLM3 3B (ONNX) - TEST MODEL",
      "repo_id": "HuggingFaceTB/SmolLM3-3B-ONNX",
      "file_path": "onnx/model.onnx",
      "model_type": "onnx",
      "size_gb": 1.5,
      "tags": ["test", "small", "onnx"],
      "suggested": false,
      "downloaded": false,
      "description": "Small 3B ONNX model for testing Python inference pipeline",
      "default_quant": "fp32",
      "source": "test",
      "requires_token": false
    },
    {
      "id": "test-gemma-3n-e4b-litert",
      "name": "Gemma 3N E4B (LiteRT) - TEST MODEL",
      "repo_id": "google/gemma-3n-E4B-it-litert-lm",
      "file_path": "model.tflite",
      "model_type": "mediapipe",
      "size_gb": 0.5,
      "tags": ["test", "mediapipe", "google", "tiny"],
      "suggested": false,
      "downloaded": false,
      "description": "Google Gemma LiteRT model for MediaPipe testing - may require HuggingFace token",
      "default_quant": "int8",
      "source": "test",
      "requires_token": true
    },
    {
      "id": "qwen3-30b-a3b-q4-k-m",
      "name": "Qwen3 30B A3B (Q4_K_M) - DEFAULT",
      "repo_id": "unsloth/Qwen3-30B-A3B-GGUF",
      "file_path": "Qwen3-30B-A3B-Q4_K_M.gguf",
      "model_type": "gguf",
      "size_gb": 18.0,
      "tags": ["default", "large", "quality", "instruct", "reasoning"],
      "suggested": true,
      "downloaded": false,
      "description": "Qwen3 30B optimized by Unsloth - DEFAULT GGUF MODEL with excellent quality and reasoning",
      "default_quant": "q4_k_m",
      "source": "builtin"
    },
    {
      "id": "qwen2.5-1.5b-instruct-q4",
      "name": "Qwen 2.5 1.5B Instruct (Q4)",
      "repo_id": "Qwen/Qwen2.5-1.5B-Instruct-GGUF",
      "file_path": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "model_type": "gguf",
      "size_gb": 1.0,
      "tags": ["fast", "tiny", "instruct"],
      "suggested": true,
      "downloaded": false,
      "description": "Ultra-fast 1.5B model perfect for quick responses and low-resource environments",
      "default_quant": "q4_k_m",
      "source": "builtin"
    },
    {
      "id": "phi-3.5-mini-instruct-q4",
      "name": "Phi-3.5 Mini Instruct (Q4)",
      "repo_id": "microsoft/Phi-3.5-mini-instruct-onnx",
      "file_path": "onnx/model_q4f16.onnx",
      "model_type": "onnx",
      "size_gb": 2.3,
      "tags": ["fast", "small", "instruct", "microsoft"],
      "suggested": true,
      "downloaded": false,
      "description": "Microsoft's efficient Phi-3.5 model optimized for ONNX runtime",
      "default_quant": "q4f16",
      "source": "builtin"
    },
    {
      "id": "llama-3.2-3b-instruct-q4",
      "name": "Llama 3.2 3B Instruct (Q4)",
      "repo_id": "hugging-quants/Llama-3.2-3B-Instruct-Q4_K_M-GGUF",
      "file_path": "llama-3.2-3b-instruct-q4_k_m.gguf",
      "model_type": "gguf",
      "size_gb": 2.0,
      "tags": ["medium", "instruct", "meta"],
      "suggested": true,
      "downloaded": false,
      "description": "Meta's Llama 3.2 3B model, balanced performance and size",
      "default_quant": "q4_k_m",
      "source": "builtin"
    },
    {
      "id": "llama-3.1-8b-instruct-q4",
      "name": "Llama 3.1 8B Instruct (Q4)",
      "repo_id": "bartowski/Meta-Llama-3.1-8B-Instruct-GGUF",
      "file_path": "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
      "model_type": "gguf",
      "size_gb": 4.9,
      "tags": ["large", "quality", "instruct", "meta"],
      "suggested": true,
      "downloaded": false,
      "description": "High-quality 8B model for complex tasks and longer contexts",
      "default_quant": "q4_k_m",
      "source": "builtin"
    },
    {
      "id": "deepseek-r1-distill-llama-8b-q4",
      "name": "DeepSeek R1 Distill Llama 8B (Q4)",
      "repo_id": "bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF",
      "file_path": "DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf",
      "model_type": "gguf",
      "size_gb": 4.9,
      "tags": ["reasoning", "large", "instruct"],
      "suggested": true,
      "downloaded": false,
      "description": "DeepSeek R1 distilled model optimized for reasoning tasks",
      "default_quant": "q4_k_m",
      "source": "builtin"
    },
    {
      "id": "falcon3-1b-instruct-1.58bit",
      "name": "Falcon3 1B Instruct (1.58-bit)",
      "repo_id": "tiiuae/Falcon3-1B-Instruct-1.58bit",
      "file_path": null,
      "model_type": "bitnet",
      "size_gb": 0.2,
      "tags": ["ultra-fast", "efficient", "tiny", "1.58bit", "bitnet"],
      "suggested": true,
      "downloaded": false,
      "description": "Ultra-efficient 1.58-bit quantized model, extremely fast with minimal resources",
      "default_quant": "i2_s",
      "source": "builtin"
    },
    {
      "id": "falcon3-3b-instruct-1.58bit",
      "name": "Falcon3 3B Instruct (1.58-bit)",
      "repo_id": "tiiuae/Falcon3-3B-Instruct-1.58bit",
      "file_path": null,
      "model_type": "bitnet",
      "size_gb": 0.4,
      "tags": ["ultra-fast", "efficient", "small", "1.58bit", "bitnet"],
      "suggested": true,
      "downloaded": false,
      "description": "3B BitNet model with extreme efficiency and speed",
      "default_quant": "i2_s",
      "source": "builtin"
    },
    {
      "id": "falcon3-7b-instruct-1.58bit",
      "name": "Falcon3 7B Instruct (1.58-bit)",
      "repo_id": "tiiuae/Falcon3-7B-Instruct-1.58bit",
      "file_path": null,
      "model_type": "bitnet",
      "size_gb": 0.9,
      "tags": ["fast", "efficient", "medium", "1.58bit", "bitnet"],
      "suggested": true,
      "downloaded": false,
      "description": "7B BitNet model, excellent balance of quality and efficiency",
      "default_quant": "i2_s",
      "source": "builtin"
    },
    {
      "id": "llama3-8b-1.58bit",
      "name": "Llama3 8B (1.58-bit)",
      "repo_id": "HF1BitLLM/Llama3-8B-1.58-100B-tokens",
      "file_path": null,
      "model_type": "bitnet",
      "size_gb": 1.0,
      "tags": ["fast", "efficient", "large", "1.58bit", "bitnet", "meta"],
      "suggested": true,
      "downloaded": false,
      "description": "Llama3 8B with BitNet quantization, trained on 100B tokens",
      "default_quant": "i2_s",
      "source": "builtin"
    },
    {
      "id": "bitnet-b1.58-3b",
      "name": "BitNet b1.58 3B",
      "repo_id": "1bitLLM/bitnet_b1_58-3B",
      "file_path": null,
      "model_type": "bitnet",
      "size_gb": 0.5,
      "tags": ["ultra-fast", "efficient", "small", "1.58bit", "bitnet"],
      "suggested": true,
      "downloaded": false,
      "description": "Original BitNet b1.58 3B model with extreme efficiency",
      "default_quant": "i2_s",
      "source": "builtin"
    },
    {
      "id": "bitnet-b1.58-large",
      "name": "BitNet b1.58 Large",
      "repo_id": "1bitLLM/bitnet_b1_58-large",
      "file_path": null,
      "model_type": "bitnet",
      "size_gb": 1.5,
      "tags": ["fast", "efficient", "large", "1.58bit", "bitnet"],
      "suggested": false,
      "downloaded": false,
      "description": "Larger BitNet b1.58 model for better quality",
      "default_quant": "i2_s",
      "source": "builtin"
    },
    {
      "id": "bitnet-b1.58-2b-4t",
      "name": "BitNet b1.58 2B (4T)",
      "repo_id": "microsoft/BitNet-b1.58-2B-4T",
      "file_path": null,
      "model_type": "bitnet",
      "size_gb": 0.3,
      "tags": ["microsoft", "efficient", "small", "1.58bit", "bitnet"],
      "suggested": false,
      "downloaded": false,
      "description": "Microsoft's BitNet 2B model trained on 4T tokens",
      "default_quant": "i2_s",
      "source": "builtin"
    },
    {
      "id": "bartowski-llama-3.2-3b-instruct-q4",
      "name": "Llama 3.2 3B Instruct (Bartowski Q4)",
      "repo_id": "bartowski/Llama-3.2-3B-Instruct-GGUF",
      "file_path": "Llama-3.2-3B-Instruct-Q4_K_M.gguf",
      "model_type": "gguf",
      "size_gb": 2.0,
      "tags": ["popular", "medium", "instruct", "meta"],
      "suggested": true,
      "downloaded": false,
      "description": "Popular Llama 3.2 3B by Bartowski - 262k downloads",
      "default_quant": "q4_k_m",
      "source": "builtin"
    },
    {
      "id": "unsloth-gpt-oss-20b-q4",
      "name": "GPT-OSS 20B (Unsloth Q4)",
      "repo_id": "unsloth/gpt-oss-20b-GGUF",
      "file_path": "gpt-oss-20b-Q4_K_M.gguf",
      "model_type": "gguf",
      "size_gb": 12.0,
      "tags": ["popular", "large", "quality"],
      "suggested": true,
      "downloaded": false,
      "description": "Open-source GPT 20B by Unsloth - 211k downloads",
      "default_quant": "q4_k_m",
      "source": "builtin"
    },
    {
      "id": "unsloth-qwen3-coder-30b-instruct-q4",
      "name": "Qwen3 Coder 30B Instruct (Q4)",
      "repo_id": "unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF",
      "file_path": "Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf",
      "model_type": "gguf",
      "size_gb": 18.0,
      "tags": ["coding", "large", "quality", "instruct"],
      "suggested": true,
      "downloaded": false,
      "description": "Qwen3 Coder 30B specialized for coding tasks - 162k downloads",
      "default_quant": "q4_k_m",
      "source": "builtin"
    },
    {
      "id": "google-gemma-7b-it-q4",
      "name": "Google Gemma 7B IT (Q4)",
      "repo_id": "google/gemma-7b-it",
      "file_path": "gemma-7b-it-Q4_K_M.gguf",
      "model_type": "gguf",
      "size_gb": 4.5,
      "tags": ["google", "popular", "medium", "instruct"],
      "suggested": true,
      "downloaded": false,
      "description": "Google's Gemma 7B instruction-tuned - 158k downloads, 1.21k likes",
      "default_quant": "q4_k_m",
      "source": "builtin"
    },
    {
      "id": "unsloth-deepseek-r1-qwen3-8b-q4",
      "name": "DeepSeek R1 Qwen3 8B (Q4)",
      "repo_id": "unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF",
      "file_path": "DeepSeek-R1-0528-Qwen3-8B-Q4_K_M.gguf",
      "model_type": "gguf",
      "size_gb": 5.0,
      "tags": ["reasoning", "large", "quality"],
      "suggested": true,
      "downloaded": false,
      "description": "DeepSeek R1 reasoning model based on Qwen3 - 113k downloads",
      "default_quant": "q4_k_m",
      "source": "builtin"
    },
    {
      "id": "modularai-llama-3.1-8b-instruct-q4",
      "name": "Llama 3.1 8B Instruct (ModularAI Q4)",
      "repo_id": "modularai/Llama-3.1-8B-Instruct-GGUF",
      "file_path": "Llama-3.1-8B-Instruct-Q4_K_M.gguf",
      "model_type": "gguf",
      "size_gb": 5.0,
      "tags": ["modular", "large", "instruct", "meta"],
      "suggested": true,
      "downloaded": false,
      "description": "ModularAI optimized Llama 3.1 8B - 108k downloads",
      "default_quant": "q4_k_m",
      "source": "builtin"
    },
    {
      "id": "maziyarpanahi-phi-3.5-mini-q4",
      "name": "Phi-3.5 Mini Instruct (Maziyar Q4)",
      "repo_id": "MaziyarPanahi/Phi-3.5-mini-instruct-GGUF",
      "file_path": "Phi-3.5-mini-instruct.Q4_K_M.gguf",
      "model_type": "gguf",
      "size_gb": 2.5,
      "tags": ["fast", "small", "microsoft"],
      "suggested": true,
      "downloaded": false,
      "description": "Microsoft Phi-3.5 Mini by Maziyar - 104k downloads",
      "default_quant": "q4_k_m",
      "source": "builtin"
    },
    {
      "id": "maziyarpanahi-qwen3-14b-q4",
      "name": "Qwen3 14B (Maziyar Q4)",
      "repo_id": "MaziyarPanahi/Qwen3-14B-GGUF",
      "file_path": "Qwen3-14B.Q4_K_M.gguf",
      "model_type": "gguf",
      "size_gb": 8.5,
      "tags": ["large", "quality"],
      "suggested": false,
      "downloaded": false,
      "description": "Qwen3 14B by Maziyar - 97.4k downloads",
      "default_quant": "q4_k_m",
      "source": "builtin"
    },
    {
      "id": "maziyarpanahi-llama-3-8b-instruct-q4",
      "name": "Llama 3 8B Instruct (Maziyar Q4)",
      "repo_id": "MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF",
      "file_path": "Meta-Llama-3-8B-Instruct.Q4_K_M.gguf",
      "model_type": "gguf",
      "size_gb": 4.9,
      "tags": ["large", "instruct", "meta"],
      "suggested": false,
      "downloaded": false,
      "description": "Meta Llama 3 8B by Maziyar - 96.1k downloads",
      "default_quant": "q4_k_m",
      "source": "builtin"
    },
    {
      "id": "maziyarpanahi-mistral-7b-v0.3-q4",
      "name": "Mistral 7B Instruct v0.3 (Maziyar Q4)",
      "repo_id": "MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF",
      "file_path": "Mistral-7B-Instruct-v0.3.Q4_K_M.gguf",
      "model_type": "gguf",
      "size_gb": 4.4,
      "tags": ["medium", "instruct", "mistral"],
      "suggested": true,
      "downloaded": false,
      "description": "Mistral 7B v0.3 by Maziyar - 93.9k downloads",
      "default_quant": "q4_k_m",
      "source": "builtin"
    },
    {
      "id": "bartowski-llama-3.1-8b-instruct-q4",
      "name": "Llama 3.1 8B Instruct (Bartowski Q4)",
      "repo_id": "bartowski/Meta-Llama-3.1-8B-Instruct-GGUF",
      "file_path": "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
      "model_type": "gguf",
      "size_gb": 4.9,
      "tags": ["popular", "large", "instruct", "meta"],
      "suggested": true,
      "downloaded": false,
      "description": "Bartowski's Llama 3.1 8B - 80.9k downloads, highly liked",
      "default_quant": "q4_k_m",
      "source": "builtin"
    },
    {
      "id": "maziyarpanahi-mistral-nemo-q4",
      "name": "Mistral Nemo Instruct 2407 (Maziyar Q4)",
      "repo_id": "MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF",
      "file_path": "Mistral-Nemo-Instruct-2407.Q4_K_M.gguf",
      "model_type": "gguf",
      "size_gb": 7.5,
      "tags": ["large", "instruct", "mistral"],
      "suggested": false,
      "downloaded": false,
      "description": "Mistral Nemo 12B instruction model - 81k downloads",
      "default_quant": "q4_k_m",
      "source": "builtin"
    }
  ]
}

