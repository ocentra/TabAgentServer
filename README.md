# TabAgent Server

**Multi-modal Intelligent Assistant (MIA) - Agentic AI Platform**

Unified inference infrastructure powering the [TabAgent browser extension](https://github.com/ocentra/TabAgent) and future agentic systems. Combines Rust performance with Python ML for vision, language, and audio understanding.

**What is MIA?** A cognitive architecture that remembers, learns, and acts‚Äînot just a model server. Think of it as a brain with multiple memory systems (7 databases), learning from experience, and making intelligent decisions using eyes (vision), ears (audio), and reasoning (LLMs).

**Learn More**:
- üéØ [MIA_VISION.md](MIA_VISION.md) - **What we're building** (vision document, "show and tell")
- üß† [Rust/docs/mia_memory.md](Rust/docs/mia_memory.md) - **How it works** (complete technical architecture)

---

## Architecture

```
TabAgent Server
‚îú‚îÄ‚îÄ Rust/           ‚Üí Core inference infrastructure (WebRTC, gRPC, Database, API)
‚îú‚îÄ‚îÄ PythonML/       ‚Üí ML services (MediaPipe, Transformers, LiteRT) via gRPC
‚îú‚îÄ‚îÄ External/       ‚Üí Third-party integrations (BitNet, MediaPipe source)
‚îî‚îÄ‚îÄ Scripts/        ‚Üí Build and setup automation
```

---

## Key Capabilities

## MIA Vision: Multi-modal Agentic AI

**Beyond Text-Only LLMs** - MIA agents will see, hear, and understand the world:

üéØ **Vision Agents** (MediaPipe + Computer Vision)
- Real-time face/hand/pose tracking
- Gesture recognition for UI control
- Scene understanding & object detection
- Gaze estimation for attention tracking
- **Agents decide with eyes, not just text**

üó£Ô∏è **Audio Agents** (Whisper + Speech)
- Real-time transcription & translation
- Voice commands & speaker recognition
- Audio scene analysis
- **Agents listen and respond naturally**

ü§ñ **Language Agents** (Transformers + LiteRT)
- Multi-turn reasoning & chat
- Code generation & analysis
- Multi-modal understanding (Florence2, CLIP)
- 1.58-bit BitNet (50 tok/s on CPU!)
- **Agents think and communicate**

üíæ **Cognitive Memory** (7 Databases)
- Conversations (episodic memory)
- Knowledge graph (semantic memory)
- Tool results (external knowledge cache)
- **Experience** (learning from feedback)
- Embeddings (similarity search)
- Meta-memory (knows what it knows)
- **Agents remember and learn**

üîß **Tool Use & Learning**
- Web search, scraping, APIs
- Action outcome tracking
- User feedback integration
- Success/failure pattern recognition
- **Agents improve from experience**

‚ö° **Performance**
- Hardware-aware (CPU/GPU/NPU auto-detection)
- BitNet 1.58-bit (all platforms)
- Streaming inference
- VRAM-aware offloading

---

## Quick Start

### Prerequisites
- **Rust**: 1.75+ (`rustup`)
- **Python**: 3.10+ with pip
- **Node.js**: 18+ (for extension, optional)
- **GPU**: NVIDIA/AMD/Intel (optional, auto-detected)

### Setup

```bash
# 1. Clone repository
git clone https://github.com/yourusername/TabAgent
cd TabAgent/TabAgentServer

# 2. Install Python dependencies
cd PythonML
pip install -r requirements.txt
python -m grpc_tools.protoc -I../Rust/protos --python_out=generated --grpc_python_out=generated ../Rust/protos/*.proto
cd ..

# 3. Build Rust server
cd Rust
cargo build --release

# 4. Run server (starts both Rust + Python)
cargo run --bin tabagent-server -- --mode all
```

Server starts on:
- **HTTP API**: http://localhost:3000
- **WebRTC**: http://localhost:8002
- **Python ML gRPC**: localhost:50051 (internal)

### Test MediaPipe

```bash
cd PythonML
pytest tests/test_mediapipe.py -v
```

---

## Project Structure

### [`PythonML/`](PythonML/README.md) - ML Services
Python ML stack running as gRPC subprocess managed by Rust.

**Modules**:
- `services/` - gRPC service implementations
- `mediapipe/` - Vision/pose tracking (7 specialized modules)
- `pipelines/` - HuggingFace Transformers (15 pipeline types)
- `litert/` - Quantized edge models
- `core/` - File provider, stream handling

**Communication**: Rust spawns Python, communicates via gRPC (port 50051)

---

### [`Rust/`](Rust/README.md) - Core Infrastructure
High-performance inference orchestration and system integration.

**Key Crates**:
- `server` - Main server binary (HTTP + WebRTC + Native)
- `api` - REST API routes with OpenAPI
- `appstate` - Application state + model orchestrator
- `storage` - Database layer (MIA memory system)
- `common` - Shared types, gRPC clients, platform utils
- `model-cache` - Model download & management
- `webrtc` - WebRTC signaling & data channels
- `native-messaging` - Chrome extension protocol
- `hardware` - Auto-detection (CPU/GPU/NPU)
- `onnx-loader`, `gguf-loader` - Model loaders
- `pipeline` - Inference orchestration

**See**: [Rust/README.md](Rust/README.md) for all crates

---

## Communication Flow

```
Chrome Extension
    ‚Üì (Native Messaging / WebRTC)
Rust Server (port 3000/8002)
    ‚Üì (gRPC - localhost:50051)
Python ML Service
    ‚Üì (MediaPipe / Transformers / LiteRT)
Hardware (CPU/GPU/NPU)
```

**Key Points**:
- Rust is the orchestrator and "brain"
- Python is a stateless ML slave
- gRPC enables language-agnostic communication
- Rust can run locally or call remote Python

---

## Features

### Vision AI (MediaPipe)
- ‚úÖ Face detection (6 keypoints)
- ‚úÖ Face mesh (468 landmarks, 3D)
- ‚úÖ Hand tracking (21 landmarks + 7 gestures)
- ‚úÖ Pose tracking (33 landmarks + angles)
- ‚úÖ Holistic tracking (543 landmarks combined)
- ‚úÖ Iris tracking (gaze estimation)
- ‚úÖ Segmentation (person/background + effects)

### Language Models (Transformers)
- ‚úÖ Text generation (streaming)
- ‚úÖ Embeddings (sentence-transformers)
- ‚úÖ Chat completion
- ‚úÖ Multi-modal (Florence2, CLIP, Whisper)
- ‚öôÔ∏è All 15 pipelines (in progress)

### Edge Models (LiteRT + BitNet)
- ‚úÖ BitNet 1.58-bit (CPU-optimized, all platforms)
- ‚öôÔ∏è Quantized Gemma models (LiteRT)
- ‚öôÔ∏è XNNPACK/GPU acceleration

### Database (Storage)
- ‚úÖ 7-database MIA architecture
- ‚úÖ gRPC service for remote access
- ‚úÖ Vector embeddings
- ‚úÖ Graph queries

### WebRTC
- ‚úÖ Signaling server
- ‚úÖ Data channels
- ‚úÖ Video stream processing
- ‚úÖ Browser demos

---

## Development

### Running Tests

```bash
# Python tests
cd PythonML
pytest -v

# Rust tests
cd Rust
cargo test --workspace

# Integration tests
cargo test --test '*' -- --test-threads=1
```

### Building

```bash
# Development
cd Rust
cargo build

# Release (optimized)
cargo build --release

# Specific mode
cargo run --bin tabagent-server -- --mode web --port 3000
```

### Server Modes

- `native` - Native messaging only (for extension)
- `http` - HTTP API only
- `webrtc` - WebRTC signaling only
- `web` - HTTP + WebRTC (no native messaging)
- `all` - Everything (default)

---

## Documentation

### Vision & Architecture
- **[MIA_VISION.md](MIA_VISION.md)** - üéØ **What we're building** (vision document, accessible overview)
- **[Rust/docs/mia_memory.md](Rust/docs/mia_memory.md)** - üß† **MIA Cognitive Architecture** (complete technical design, 7 databases)

### Component Documentation
- **[PythonML/README.md](PythonML/README.md)** - Python ML services architecture
- **[Rust/README.md](Rust/README.md)** - Rust infrastructure overview
- **[Rust/GRPC_ARCHITECTURE.md](Rust/GRPC_ARCHITECTURE.md)** - gRPC communication design
- **[Rust/docs/](Rust/docs/)** - Database layer specs, query engine, knowledge weaver

### Module Documentation
Each module has:
- `README.md` - Architecture, usage, examples
- `TODO.md` - Current state, planned features

---

## Performance

| Configuration | First Token | Throughput | Memory |
|--------------|-------------|------------|--------|
| MediaPipe (face mesh) | 15ms | 60 FPS | 200MB RAM |
| Transformers (7B Q4) | 80ms | 35 tok/s | 6GB VRAM |
| LiteRT (Gemma 3B) | 50ms | 45 tok/s | 4GB VRAM |
| **BitNet (3B 1.58-bit)** | **40ms** | **50 tok/s** | **2GB RAM** ‚úÖ |

*NVIDIA RTX 4090 + i9-12900K*

---

## Platform Support

| Platform | Status | Notes |
|----------|--------|-------|
| Windows | ‚úÖ Complete | Full hardware detection, DirectML |
| Linux | ‚úÖ Complete | CUDA/ROCm support |
| macOS | ‚úÖ Complete | Metal acceleration |

### BitNet Support
‚úÖ **1.58-bit quantization** across all platforms:
- **CPU**: x86 (SSE, AVX2, AVX512), ARM (NEON)
- **GPU**: NVIDIA (CUDA), AMD (ROCm), Intel (OpenCL)
- **Performance**: 50 tok/s @ 3B model on CPU (no GPU needed!)

---

## License

Apache 2.0 - See [LICENSE](LICENSE)

---

## Project Context

**Primary Purpose**: Powers the [TabAgent browser extension](https://github.com/ocentra/TabAgent) with AI capabilities.

**Vision**: Not limited to browser automation‚ÄîMIA is a unified multi-modal AI platform for:
- Browser assistants (TabAgent)
- Desktop AI agents (future)
- Voice assistants (future)
- Vision-based automation (future)
- Any application needing cognitive AI

**What Makes MIA Different**:
- **Multi-modal by design**: Vision + Audio + Text from day one
- **Cognitive architecture**: 7-database memory system that learns
- **True agents**: Not just models‚Äîagents that see, hear, remember, learn, and act
- **Production-ready**: Real implementations, no stubs, enterprise-grade

## Contributing

See individual module READMEs for contribution guidelines:
- [PythonML/README.md](PythonML/README.md)
- [Rust/README.md](Rust/README.md)

---

## System Requirements

**Minimum**:
- 8GB RAM
- 4-core CPU
- 10GB disk space

**Recommended**:
- 16GB RAM
- NVIDIA/AMD GPU with 8GB+ VRAM
- 50GB disk space (for models)

**Models stored in**:
- Windows: `%APPDATA%/TabAgent/models/`
- Linux: `~/.local/share/TabAgent/models/`
- macOS: `~/Library/Application Support/TabAgent/models/`
