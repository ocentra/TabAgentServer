# TabAgent Backends

**Inference backend implementations - Python & Rust**

---

## Architecture Overview

TabAgent uses a **DRY architecture** where:
1. **Rust** handles ALL detection logic (format + task)
2. **Python** executes inference based on Rust's routing decision
3. **No duplication** of detection/routing logic

---

## Backend Types

### **Rust Backends** (High-performance, FFI-based)
- âœ… **BitNet** - 1.58-bit quantized models (bitnet.dll)
- âœ… **GGUF** - Standard quantized models (llama.cpp)
- ğŸ”œ **ONNX** - Multi-provider (migrating from Python)
- ğŸ”œ **LiteRT** - On-device models (migrating from Python)

### **Python Backends** (Library-based)
- âœ… **Transformers** - PyTorch/SafeTensors (HuggingFace)
- â³ **ONNX** - onnxruntime (temporary, migrating to Rust)
- â³ **MediaPipe** - LiteRT models (temporary, migrating to Rust)

---

## Structure

```
backends/
â”œâ”€â”€ base_backend.py         # Abstract base class for all backends
â”œâ”€â”€ transformers_backend.py # PyTorch/SafeTensors inference (NEW!)
â”‚
â”œâ”€â”€ bitnet/                 # BitNet 1.58 backend [Rust]
â”‚   â”œâ”€â”€ manager.py          # Uses BitNet/Release/cpu & gpu
â”‚   â””â”€â”€ validator.py        # Model type detection
â”‚
â”œâ”€â”€ llamacpp/               # General GGUF backend [Rust]
â”‚   â”œâ”€â”€ manager.py          # Uses BitNet/Release/cpu (standard binary)
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ onnxrt/                 # ONNX Runtime backend [Python â†’ Rust]
â”‚   â”œâ”€â”€ manager.py          # DirectML, CUDA, NPU support
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ mediapipe/              # MediaPipe backend [Python â†’ Rust]
â”‚   â””â”€â”€ manager.py
â”‚
â””â”€â”€ lmstudio/               # LM Studio proxy
    â””â”€â”€ manager.py
```

---

## Binary Location (IMPORTANT!)

All backends now use `BitNet/Release/` for binaries:

### CPU Binaries
```
BitNet/Release/cpu/
â”œâ”€â”€ linux/
â”‚   â”œâ”€â”€ llama-server-bitnet      # BitNet 1.58 optimized (TL2)
â”‚   â””â”€â”€ llama-server-standard    # Regular GGUF (Vulkan)
â”œâ”€â”€ macos/
â”‚   â”œâ”€â”€ llama-server-bitnet      # BitNet 1.58 optimized (TL1)
â”‚   â””â”€â”€ llama-server-standard    # Regular GGUF (Metal)
â””â”€â”€ windows/
    â”œâ”€â”€ llama-server-bitnet.exe  # BitNet 1.58 optimized (TL2)
    â””â”€â”€ llama-server-standard.exe # Regular GGUF (CUDA/Vulkan)
```

### GPU Modules
```
BitNet/Release/gpu/
â”œâ”€â”€ linux/
â”‚   â”œâ”€â”€ bitlinear_cuda.so
â”‚   â””â”€â”€ *.py (model, generate, tokenizer, etc.)
â”œâ”€â”€ macos/
â”‚   â””â”€â”€ *.py (no CUDA - Python only)
â””â”€â”€ windows/
    â”œâ”€â”€ bitlinear_cuda.pyd
    â””â”€â”€ *.py
```

---

## How Binaries Are Selected

### BitNetManager
```python
# Automatically chooses:
model_type = detect_model_type(model_path)

if model_type == ModelType.BITNET_158:
    binary = "llama-server-bitnet"    # 2-6x faster!
else:
    binary = "llama-server-standard"  # CUDA/Vulkan/Metal
```

### LlamaCppManager
```python
# Always uses standard:
binary = "llama-server-standard"  # Full GPU support
```

---

## Build Status

| Platform | CPU Binary | GPU Kernel | Status |
|----------|-----------|------------|--------|
| **Linux** | âœ… Built (WSL2) | âœ… Built | Ready |
| **macOS** | âœ… Built (Actions) | âœ… Built | Ready |
| **Windows** | â³ Pending | â³ Pending | Build locally |

Build Windows with: `cd BitNet && build-all-windows.bat`

---

## Why This Structure?

### Old (Messy):
```
backends/bitnet/binaries/
â”œâ”€â”€ macos/
â”‚   â”œâ”€â”€ cpu-macos/     â† Nested!
â”‚   â”‚   â””â”€â”€ binary
â”‚   â””â”€â”€ gpu-macos/     â† Nested!
â””â”€â”€ (no linux, no windows)
```

### New (Clean):
```
BitNet/Release/
â”œâ”€â”€ cpu/{platform}/binary    â† Flat!
â””â”€â”€ gpu/{platform}/modules   â† Flat!
```

**Benefits**:
- âœ… Single source of truth
- âœ… CI/CD populates directly
- âœ… Easy to manage
- âœ… Clear separation (cpu vs gpu)
- âœ… No nesting confusion

---

**Updated**: All backends now point to `BitNet/Release/`  
**Status**: Ready for Linux/macOS, Windows pending build

---

## Routing Flow (DRY Architecture)

### 1. Detection (Rust)
```rust
// detection.rs - Single source of truth
detect_model_py(source, auth_token) â†’ {
    "model_type": "SafeTensors",
    "backend": { "Python": { "engine": "transformers" } },
    "task": "text-generation",
    "repo": "microsoft/phi-2"
}
```

### 2. Routing (Python)
```python
# native_host.py - Routes based on Rust's decision
if backend["Python"]["engine"] == "transformers":
    load_transformers_python(source, task, token)
elif backend["Rust"]["engine"] == "llama.cpp":
    rust_handle_message({"action": "load_model", "modelPath": source})
```

### 3. Execution (Backend-specific)
```python
# backends/transformers_backend.py
TransformersTextGenBackend().load_model(model_path, task)
â†’ Uses torch, transformers, accelerate
```

---

## Migration Plan (Python â†’ Rust)

### Current State
| Backend | Implementation | Status |
|---------|---------------|--------|
| BitNet | Rust (bitnet.dll) | âœ… Stable |
| GGUF | Rust (llama.cpp) | âœ… Stable |
| Transformers | Python (HuggingFace) | âœ… Stable |
| ONNX | Python (onnxruntime) | â³ Migrating |
| LiteRT | Python (mediapipe) | â³ Migrating |

### Migration Flags (native_host.py)
```python
class Config:
    ONNX_USE_RUST = False    # Set True when Rust ONNX ready
    LITERT_USE_RUST = False  # Set True when Rust LiteRT ready
```

### Target State (Future)
| Backend | Implementation | Performance Gain |
|---------|---------------|-----------------|
| BitNet | Rust (bitnet.dll) | Current |
| GGUF | Rust (llama.cpp) | Current |
| Transformers | Python (HuggingFace) | Stays in Python |
| ONNX | Rust (onnxruntime-rs) | 2-3x faster |
| LiteRT | Rust (mediapipe-rs) | 2-3x faster |

**Why migrate ONNX/LiteRT but not Transformers?**
- ONNX/LiteRT: C++ libraries â†’ natural fit for Rust FFI
- Transformers: Deep Python ecosystem â†’ better in Python

